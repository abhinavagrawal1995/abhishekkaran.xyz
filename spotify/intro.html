<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js">
<!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title></title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0-rc.2/css/materialize.min.css">
    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0-rc.2/js/materialize.min.js"></script>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
</head>

<body>
    <!--[if lt IE 7]>
            <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="#">upgrade your browser</a> to improve your experience.</p>
        <![endif]-->

    <h1 align="center">
        <a href="/spotify/index.html">Spotify Recommendation Engine</a>
    </h1>
    <hr>
    <br/>
    <div align="center">
        <a href="/spotify/eda.html">EDA</a> |
        <a href="/spotify/modeling.html">Data Cleaning & Modeling</a> |
        <a href="/spotify/human_results.html">Human Results</a> |
        <a href="/spotify/future_work.html">Future Work</a> |
        <a href="/spotify/contact.html">Contact Us</a>
    </div>
    <br/>
    <h3 align="center">Introduction</h3>
    <br/>
    <div style="padding-left: 100px;padding-right: 100px;">
        <b>Goal (broad):</b>
        <br/><p>The goal of this project was to take some form of a user-provided list of Spotify songs, and return to the
        user a given number of song recommendations based on preferences “learned” from the songs they originally provided
        us. </p>
        <b>Data collection: </b>
        <br/><p>In order to understand our thought process we must first address the subject of data collection,
        because it shaped the way we approached the project. We were given user data in the form of 1 million playlists,
        and we knew from the beginning the we would be unable to scrape an extensive amount of data from the huge quantity
        supplied.</p>
        <p> 
        Spotify structures its data pertaining to each song in four objects: a track object, an artist object,
        an album object, and an audio features object. They are all relatively self explanatory in terms of the information
        stored except audio features - this object stores a wide variety of quantitative data that Spotify assigns songs,
        like energy, acousticness, etc. So the general method for data collection was to request all four objects that corresponded
        with each song in the playlist, referenced via the track URI (a unique identifier for each track). This method of
        data collection presented several problems in terms of scalability. 
        </p>
        <ol>
            <li><p>
                To get any of these objects requires an authentification
        token, but a temporary one can be easily requested from Spotify’s API. This does, however, require us to manually
        get new tokens periodically, which significantly limits our timeframe for scraping because we cannot leave a computer
        to scrape while we are at work, sleeping, etc.</p>
            </li>
            <li><p>
                Spotify will block tokens that are hitting API endpoints too frequently,
        so on quick Wifi and computers the scraping was ironically significantly more difficult, with new tokens required
        for as little as every 50 songs. We had to work around this by using slower mobile hotspots, which allowed us to
        avoid this blocking mechanism. This unfortunately comes with a side effect of a much lower request speed, albeit
        for less human maintenance.
            </p>
            </li>
            <li>
                <p>
                    There were often random stops in the scraping due to request errors; these did not occur with any pattern and we are unsure as to why they did occur.
                </p>
            </li>
        </ol>  
        <p>
         Because of the above mentioned factors, we were able
        to scrape over 32,000 songs from playlists, which amounts to 19,000 songs after removing duplicates. This is a significant
        amount of data, but without data for all songs that exist on Spotify, which would be a nearly impossible task, we
        had to design our models with certain workarounds.
        </p>
                
        <b>Goal (redefined):</b>
        <p> With a dataset of 19,000 unique songs, we had
        to think of ways to provide recommended songs without having access to all song data. This means that users could
        not just provide arbitrary songs, but had to provide songs that were in our smaller database. As a result, we added
        a caveat to our project goal. Instead, we sought to answer the question, how can we provide recommendations tailored
        to individual users’ preferences—not from songs they provide for us, but rather from songs we give to them.
        </p>
        <b>  Approach:</b>
        <p>
        We approached this question with two ideas for “algorithms.” The first was based on a user rating system. We provide
        the user randomly generated songs from our database, and in turn they give us some sort of ratings for the songs.
        We could then train a model to learn their preferences and predict what ratings they would give for any other song,
        based on the characteristics of those given songs and how they rated them. Ultimately we could give them back a list
        of any number of top rated songs from our model.
        </p>
        <p>
        For our second idea we wanted to address the fact actual music recommendation
        systems base their recommendations off songs in playlists. Users are assumed to consider these songs “good” and to
        their taste, so it is quite unorthodox to base a model off songs that users rated poorly as well (as we did in the
        first model). The issue with this, however, is that user-provided playlists could very easily have songs that were
        not in our smaller database. Thus we would be unable to actually test this model on real users, but we made it nonetheless.
        </p>
        <p>
        The models (see IPYNB for much more detail):
        <p/>
        <p> <b>Model 1:</b> The beauty of the first model is that we could test it on real
        users to see if they would like the output songs. This would require some work on their part, but we were able to
        find several people that were willing to take the time to do so. We randomly selected 20 songs (as many songs as
        we felt we could do without burdening people too much) from our database, and got ratings from 1-10 on a continuous
        scale (decimals allowed). With so many predictors in comparison to observations we begin falling victim to some degree
        of the curse of dimensionality, but there was really no way to avoid this due to real life limitations. Each model
        was fit on those 20 songs, with quantitative attributes as predictors, and the user-provided ratings as the response
        variable. Treating the response variable as continuous, we were aware we would get predictions that went under 1
        and over 10, but that would not impact our ability to take, say, the highest 5 predictions, no matter how high they
        were. We tried various model types for Model 1, with simple linear regression being the standard.
        </p>
        <p>
        <b>Model 2:</b> Rather
        than training on a dataset with only 20 observations, this model is trained on the entire database. The response
        variable is a binary classification, with 0 indicating that the song is not in the user-provided playlist, and 1
        indicating that it is. The issue with this model is therefore that depending on the size of the playlist, the number
        of classification 1’s can be very low in comparison to classification 0’s. From there we tested a multitude of classification
        models, very similar to HW6. Unfortunately, because users could not provide playlists with songs that were not in
        our database, we could not test this on user data. We did, however, randomly assign songs to be “in the playlist”
        as to check our mechanisms. 
        </p>
        <p>
        <b>*</b>note that as a result of the extremely low number of observations in Model 1, and low
        number of classifications in Model 2, we chose not to split our data into a test set. Rather, (for Model 1) we literally
        tested the model on the same human users, asking them how they liked to outputted songs.
        </p>
        <p>
        <b>**</b>also note that we chose
        not to reduce the large number of dimensions relative to observations with PCA. We did eliminate as many predictors
        as we could through other means, including highly correlated ones, but we wanted to maintain interpretability. Using
        PCA would prevent us from saying, for example, that a positive coefficient for year means that the user prefers (correlation)
        more recent songs.
        </p>

    </div>

    <div align=" right " style="margin-bottom:15px; margin-right: 60px; ">
        <a class="btn-floating btn-large waves-effect waves-light red " href="/spotify/eda.html ">
            <i class="material-icons ">navigate_next</i>
        </a>
    </div>
</body>

</html>